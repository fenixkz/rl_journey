# Cross Entropy Method

Solving RLs can be done very easily with Cross Entropy Method. While there exist many complex algorithms for model-free learning like DQN, A3C, PPO, etc. This simple, yet powerful method can be compete with these monsters. 

## Principle

Given a policy: $`\pi_\theta(a|s)`$ we sample $`N`$ trajectories from our environment in the form $`<s, a, r, s`>`$. From each trajectory we pick pair $`<s,a>`$ that resulted in reward higher than the mean of our sample. After that we fit our policy only to these __elite__ samples. 


## Mathematical Formulation (Policy Network Variant)

Let $\pi_\theta(a|s)$ be the policy parameterized by $\theta$ (e.g., the weights of your neural network). The policy defines a probability distribution over actions $a$ given a state $s$. The overall goal in RL is typically to find parameters $\theta^*$ that maximize the expected total reward obtained by following the policy. The expected total reward, or objective function, is:

$J(\theta) = E_{\tau \sim \pi_\theta} [R(\tau)] = E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]$

where:

$\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \dots)$ is a trajectory (episode).
$\tau \sim \pi_\theta$ means the trajectory is generated by starting in state $s_0$, sampling action $a_t \sim \pi_\theta(\cdot|s_t)$, and transitioning in the environment, receiving reward $r_{t+1}$.
$R(\tau)$ is the (potentially discounted) total reward for trajectory $\tau$.
$\gamma$ is the discount factor (can be 1 for undiscounted episodic tasks).
The Cross-Entropy Method provides an iterative approach to optimize $\theta$. At each iteration $k$:

Sampling: Generate a batch of $N$ trajectories $\mathcal{B}_k = \{\tau_1, \dots, \tau_N\}$ using the current policy $\pi_{\theta_k}$. Stochastic action selection (sampling from $\pi_{\theta_k}(a|s)$ rather than always taking the max probability action) is usually important here for exploration.

Evaluation: Compute the total reward $R(\tau_i)$ for each trajectory $\tau_i \in \mathcal{B}_k$.

Elite Selection: Define a performance threshold $\rho_k$. A common choice is the $(1-q)$-quantile of the rewards in the batch $\mathcal{B}_k$, where $q$ is the elite fraction (e.g., $q=0.1$ for the top 10%). Select the subset of elite trajectories:
$\mathcal{E}_k = \{ \tau_i \in \mathcal{B}_k \mid R(\tau_i) \geq \rho_k \}$

Parameter Update: The core idea connecting this to cross-entropy is that we want to update the policy parameters $\theta$ such that the policy becomes more likely to generate trajectories similar to those in the elite set $\mathcal{E}_k$. We achieve this by maximizing the likelihood of the actions taken in the elite trajectories, given the states in which they were taken. This corresponds to solving:
$\theta_{k+1} = \arg \max_\theta \sum_{\tau \in \mathcal{E}_k} \sum_{(s_t, a_t) \in \tau} \log \pi_\theta(a_t | s_t)$

This optimization problem is typically solved using standard gradient-based methods (like Adam, SGD) for one or more epochs. The objective function being maximized is the total log-likelihood of the elite state-action pairs under the policy $\pi_\theta$.

Connection to Cross-Entropy Loss: Maximizing the log-likelihood above is equivalent to minimizing the negative log-likelihood. If the action space is discrete, and $a_t$ represents the index of the action taken, while $\pi_\theta(\cdot|s_t)$ outputs probabilities (or logits) for all actions, then minimizing the negative log-likelihood:
$L(\theta) = - \sum_{\tau \in \mathcal{E}_k} \sum_{(s_t, a_t) \in \tau} \log \pi_\theta(a_t | s_t)$
is exactly equivalent to minimizing the empirical cross-entropy loss between the policy's predicted action distribution and a target distribution that assigns probability 1 to the elite action $a_t$ observed in state $s_t$.

This update step effectively minimizes the KL divergence between the distribution of trajectories induced by the updated policy $\pi_{\theta_{k+1}}$ and an idealized distribution concentrated on the high-reward trajectories found in the elite set $\mathcal{E}_k$.